{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tollycollins/Beat-Tracker-DL/blob/main/Joint_Beat_and_Tempo_Tracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t"
      ],
      "metadata": {
        "id": "DVzVeKcS8AY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh8uAm1tRuXm"
      },
      "source": [
        "\n",
        "\n",
        "# Joint Beat and Tempo Tracker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5BOznZqSVMJ"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppKUpykgUKyD",
        "outputId": "971b3312-038a-4091-c4c9-dc6e0ba5a431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# link to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86-yoT8X3oM4"
      },
      "outputs": [],
      "source": [
        "# # madmom library - Contains some code from Bock (2019)\n",
        "# !git clone --recursive https://github.com/CPJKU/madmom.git\n",
        "\n",
        "!pip install madmom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i39P6YYF36jU"
      },
      "outputs": [],
      "source": [
        "# ASAP dataset\n",
        "!git clone https://github.com/fosfrancesco/asap-dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5O73Ym8mnwD"
      },
      "outputs": [],
      "source": [
        "!pip install pretty_midi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAaw7RA6lb2R"
      },
      "outputs": [],
      "source": [
        "!pip install mir_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtaZlOzG0VPS"
      },
      "outputs": [],
      "source": [
        "# !pip install compress_pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibYT-OzBTL2c"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import h5py\n",
        "import pickle\n",
        "# from compress_pickle import dump, load\n",
        "from typing import List\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "\n",
        "import pretty_midi as pm\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display as display\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from scipy import signal\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.ndimage import maximum_filter1d as maxFilt\n",
        "import mir_eval\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "# from tensorboardcolab import *\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from madmom.features.beats import DBNBeatTrackingProcessor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrMfnrGuSR9z"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "256IRZbZvN_t"
      },
      "outputs": [],
      "source": [
        "# ASAP Metadata class - represents a dataset folder\n",
        "\n",
        "\n",
        "class ASAPMetadata:\n",
        "  \"\"\"\n",
        "  metadata: {'directory': str, \n",
        "             'names': list, \n",
        "             'original_names': str,\n",
        "             'lengths': list, \n",
        "             'features': list, \n",
        "             'sample_rate': int, \n",
        "             'tempo_range': [int, int], \n",
        "             'note_range': [int, int), \n",
        "             'rests': bool}\n",
        "\n",
        "  Note: Notes given in range 0-88 inclusive\n",
        "  Note: Tempo given in range self.tempo_range \n",
        "  \"\"\"\n",
        "\n",
        "  DEFAULT_SAMPLE_RATE = 100\n",
        "  DEFAULT_DATASET_PATH = \"./asap-dataset\"\n",
        "  DEFAULT_BASE_PATH = \"/content/gdrive/MyDrive/Colab Notebooks/QM DL for music and audio\"\n",
        "\n",
        "  DATASET_NAME = \"asap-beat-tracking\"\n",
        "  \n",
        "\n",
        "  def __init__(self, \n",
        "               json_data=None, \n",
        "               name=None, \n",
        "               base=None, \n",
        "               tempo_range=(10, 360), \n",
        "               tempo_target_type='continuous',\n",
        "               note_range=(0, 88), \n",
        "               rests=False, \n",
        "               num_files=None):\n",
        "    \"\"\"\n",
        "    tempo_range: allowable tempo range for pieces (if they have tempi outside this range, \n",
        "      they will not be added to the dataset)\n",
        "    note_range: permitted piano keyboard notes from the piece\n",
        "    rests: if True, allocate rests to top column (when no other note is playing)\n",
        "    \"\"\"\n",
        "\n",
        "    self.asap_base_path = ASAPMetadata.DEFAULT_DATASET_PATH\n",
        "    self.json_data = json_data\n",
        "    if not json_data:\n",
        "      with open(Path(self.asap_base_path, 'asap_annotations.json')) as json_file:\n",
        "        self.json_data = json.load(json_file)\n",
        "    self.original_dataset_size = len(self.json_data.keys())\n",
        "\n",
        "    # features available\n",
        "    self.features = {'pianoroll': False,\n",
        "                     'noteroll': False,\n",
        "                     'velocityroll': False,\n",
        "                     'beats': False,\n",
        "                     'downbeats': False,\n",
        "                     'tempo': False,\n",
        "                     'beatTypes': False,\n",
        "                     'timeSignatures': False,               \n",
        "                     'key_signatures': False}\n",
        "\n",
        "    # initialise with default sample rate\n",
        "    self.sample_rate = ASAPMetadata.DEFAULT_SAMPLE_RATE\n",
        "\n",
        "    self.tempo_range = tempo_range  # [low, high)\n",
        "    self.tempo_target_type = tempo_target_type\n",
        "    self.note_range = note_range\n",
        "    self.rests = rests\n",
        "\n",
        "    # set a limit on the number of files to create\n",
        "    self.num_files = num_files\n",
        "\n",
        "    self.metadata = {}\n",
        "\n",
        "    # folder path and name\n",
        "    self.base = ASAPMetadata.DEFAULT_BASE_PATH if not base else base\n",
        "    self.name = name\n",
        "    if name:\n",
        "      # scan directory for existing name directory\n",
        "      if self.folder_exists(name):\n",
        "        self.link(name)\n",
        "\n",
        "  \n",
        "  def get_metadata(self, features=None):\n",
        "    \"\"\"\n",
        "    Request metadata dictionary with given features\n",
        "    \"\"\"\n",
        "    info = self.metadata\n",
        "    if not info:\n",
        "      raise RuntimeError(\"Metadata is empty\")\n",
        "\n",
        "    if features:\n",
        "      # only keep desired features\n",
        "      for f in features:\n",
        "        if not self.features[f]:\n",
        "          raise RuntimeError(f\"feature {f} is not available \")\n",
        "      info['features'] = features\n",
        "\n",
        "    return info\n",
        "\n",
        "\n",
        "  def change_base(self, base):\n",
        "    \"\"\"\n",
        "    Change base directory attribute\n",
        "    \"\"\"\n",
        "    self.base = base\n",
        "\n",
        "\n",
        "  def link(self, name, reset_features=True):\n",
        "    \"\"\"\n",
        "    Link class to a folder name\n",
        "    \"\"\"\n",
        "    directory = self.get_path(name)\n",
        "    if not self.folder_exists(name):\n",
        "      raise FileNotFoundError(f\"Folder {directory} does not exist\")\n",
        "\n",
        "    self.name = name\n",
        "\n",
        "    # load metadata\n",
        "    self.metadata = self.load_metadata()\n",
        "\n",
        "    # update features\n",
        "    if reset_features:\n",
        "      try:\n",
        "        self.reset_features(self.metadata['features'])\n",
        "      except KeyError:\n",
        "        self.reset_features()\n",
        "\n",
        "    # update sample rate\n",
        "    try:\n",
        "      self.sample_rate = self.metadata['sample_rate']\n",
        "    except KeyError:\n",
        "      pass\n",
        "    \n",
        "    print(f\"Data Folder Manager linked to {self.get_path(name)}\")\n",
        "\n",
        "\n",
        "  def new_folder(self, name, reset_features=True):\n",
        "    \"\"\"\n",
        "    Create and link to a new folder\n",
        "    \"\"\"\n",
        "    directory = self.get_path(name)\n",
        "    if self.folder_exists(name):\n",
        "      print(f\"Folder {directory} already exists\")\n",
        "      return\n",
        "\n",
        "    os.makedirs(directory)\n",
        "    self.name = name\n",
        "    # reset metadata and features\n",
        "    self.metadata = {}\n",
        "    self.reset_features()\n",
        "\n",
        "\n",
        "  def create_data_files(self, \n",
        "                        features=None, \n",
        "                        overwrite=False, \n",
        "                        name_list=None):\n",
        "    \"\"\"\n",
        "    Dumps np arrays of examples and corresponding ground truths. \n",
        "    \"\"\"\n",
        "    path = self.get_path(self.name)\n",
        "    # check we have a valid folder name\n",
        "    if not self.folder_exists(self.name):\n",
        "      raise RuntimeError(f\"The folder {path} does not exist\")\n",
        "\n",
        "    metadata = {}\n",
        "    empty = True\n",
        "    # load metadata\n",
        "    try:\n",
        "      metadata = self.load_metadata()\n",
        "      # reset self.features\n",
        "      self.reset_features(metadata['features'])\n",
        "      empty = False\n",
        "    except KeyError:\n",
        "      self.reset_features()\n",
        "    \n",
        "    # choose features to include\n",
        "    if features:\n",
        "      for feature in features:\n",
        "        if self.features[feature] == False:\n",
        "          self.features[feature] = True\n",
        "          # force overwrite if features do not match\n",
        "          if metadata != {}:\n",
        "            overwrite = True\n",
        "      for feature, val in self.features.items():\n",
        "        if feature not in features:\n",
        "          self.features[feature] = False\n",
        "          # force overwrite if features do not match\n",
        "          if metadata != {}:\n",
        "            overwrite = True\n",
        "\n",
        "    # alter the sample rate of all data (overwrite data)\n",
        "    try:\n",
        "      if metadata['sample_rate'] != self.sample_rate:\n",
        "        overwrite = True\n",
        "    except KeyError:\n",
        "        pass\n",
        "\n",
        "    if overwrite:\n",
        "      # wipe directory\n",
        "      try:\n",
        "        shutil.rmtree(path)\n",
        "        self.new_folder(self.name, reset_features=False)\n",
        "      except OSError as e:\n",
        "        print(\"Error: %s : %s\" % (path, e.strerror))\n",
        "\n",
        "      # reset metadata\n",
        "      metadata = {'directory': path, 'names': [], 'lengths': []}\n",
        "\n",
        "    # generator for examples and labels\n",
        "    dataGen = self.gen_data(name_list=name_list)\n",
        "\n",
        "    # generate and save data\n",
        "    counter = 0\n",
        "    num_examples = 0\n",
        "    num_already = 0\n",
        "    names = []\n",
        "    original_names = []\n",
        "    lengths = []\n",
        "    for fpath, dataDict in dataGen:\n",
        "      counter += 1\n",
        "\n",
        "      # check if file limit has been reached\n",
        "      if self.num_files and counter > self.num_files:\n",
        "        break\n",
        "\n",
        "      trackname = os.path.split(fpath)[1]\n",
        "      trackname = os.path.splitext(trackname)[0]\n",
        "      # name = trackname + '.gz'\n",
        "      name = trackname + '.hdf5'\n",
        "\n",
        "      # check for duplicates\n",
        "      while name in names:\n",
        "        penult_char = name[-7]\n",
        "        suffix = '_0'\n",
        "        if penult_char == '_':\n",
        "          last_char = name[-6]\n",
        "          suffix = str(int(last_char) + 1)\n",
        "          trackname = trackname[: -1]\n",
        "        trackname += suffix\n",
        "        # name = trackname + '.gz'\n",
        "        name = trackname + '.hdf5'\n",
        "        print(f'Duplicate {name} renamed')\n",
        "\n",
        "      filename = os.path.join(path, name)\n",
        "\n",
        "      # double check for duplicates\n",
        "      if name in names:\n",
        "        print(f'Duplicate {name} not processed')\n",
        "        continue\n",
        "\n",
        "      # save data to file\n",
        "      if not os.path.isfile(filename) or overwrite:\n",
        "            \n",
        "        with h5py.File(filename, 'w') as hf:\n",
        "          hf.create_dataset('length', data=dataDict['length'])\n",
        "          if 'pianoroll' in dataDict.keys():\n",
        "            hf.create_dataset(\"pianoroll\", shape=dataDict['pianoroll'].shape, data=dataDict['pianoroll'])\n",
        "          if 'beats' in dataDict.keys():\n",
        "            hf.create_dataset(\"beats\", shape=dataDict['beats'].shape, data=dataDict['beats'])\n",
        "          if 'downbeats' in dataDict.keys():\n",
        "            hf.create_dataset(\"downbeats\", shape=dataDict['downbeats'].shape, data=dataDict['downbeats'])\n",
        "          if 'tempo' in dataDict.keys():\n",
        "            hf.create_dataset(\"tempo\", dataDict['tempo'].shape, data=dataDict['tempo'])\n",
        "\n",
        "        # with open(filename, 'wb') as f:\n",
        "        #   dump(dataDict, f)\n",
        "\n",
        "        # update metadata dict\n",
        "        names.append(name)\n",
        "        original_names.append(fpath)\n",
        "        lengths.append(dataDict['length'])\n",
        "\n",
        "        print(f'Processed {counter} out of {self.original_dataset_size}. File created: {name}')\n",
        "        num_examples += 1\n",
        "      else:\n",
        "        if empty:\n",
        "          names.append(name)\n",
        "          lengths.append(dataDict['length'])\n",
        "        num_already += 1\n",
        "\n",
        "    # save metadata\n",
        "    feat = []\n",
        "    for f, val in self.features.items():\n",
        "      if val:\n",
        "        feat.append(f)\n",
        "    metadata['names'] = names\n",
        "    metadata['original_names'] = original_names\n",
        "    metadata['lengths'] = lengths\n",
        "    metadata['features'] = feat\n",
        "    metadata['sample_rate'] = self.sample_rate\n",
        "    metadata['tempo_range'] = self.tempo_range\n",
        "    metadata['tempo_target_type'] = self.tempo_target_type\n",
        "    metadata['note_range'] = self.note_range\n",
        "    metadata['rests'] = self.rests\n",
        "    metadata['directory'] = self.get_path(self.name)\n",
        "\n",
        "    save_path = os.path.join(self.get_path(self.name), 'metadata.json')\n",
        "    with open(save_path, 'w') as fp:\n",
        "      json.dump(metadata, fp)    \n",
        "    self.metadata = metadata\n",
        "\n",
        "    print(f\"\\n {num_examples} files created and {num_already} already present \" \\\n",
        "          f\"out of {self.original_dataset_size} originals\")\n",
        "    \n",
        "\n",
        "  def gen_data(self, name_list=None):\n",
        "    \"\"\"\n",
        "    generator for MIDI piano rolls and asociated ground truths from MIDI files\n",
        "    \"\"\"\n",
        "    for path in self.json_data.keys():\n",
        "\n",
        "      # check track name (for selecting test samples)\n",
        "      if name_list is not None:\n",
        "        trackname = os.path.split(path)[1]\n",
        "        trackname = os.path.splitext(trackname)[0]\n",
        "        if trackname not in name_list:\n",
        "          continue\n",
        "\n",
        "      data = {}\n",
        "\n",
        "      # get next piano roll\n",
        "      piece = pm.PrettyMIDI(os.path.join(self.asap_base_path, path)).get_piano_roll(100)\n",
        "\n",
        "      # slice to note range\n",
        "      piece = piece[slice(*self.note_range), :]\n",
        "\n",
        "      if self.rests:\n",
        "        rests = np.where(np.any(piece, axis=0, keepdims=True), 0, 1)\n",
        "        piece = np.concatenate((rests, piece))\n",
        "      \n",
        "      # change data type\n",
        "      piece = piece.astype(np.int8)\n",
        "\n",
        "      if self.features['pianoroll']:\n",
        "        data['pianoroll'] = piece\n",
        "\n",
        "      num_time_points = piece.shape[1]\n",
        "      data['length'] = num_time_points\n",
        "\n",
        "      # get associated beat and downbeat labels\n",
        "      beats = self.json_data[path][\"performance_beats\"]\n",
        "      downbeats = self.json_data[path][\"performance_downbeats\"]\n",
        "      # get positions of '1's in labels arrays\n",
        "      # note: sample rate set to 100\n",
        "      beatPositions = np.multiply(np.around(beats, decimals=2), 100).astype(int)\n",
        "      downbeatPositions = np.multiply(np.around(downbeats, decimals=2), 100).astype(int)\n",
        "\n",
        "      # make beat and downbeat labels\n",
        "      if self.features['beats']:\n",
        "        data['beats'] = self.make_beat_annotations(beatPositions, num_time_points)\n",
        "      if self.features['downbeats']:\n",
        "        data['downbeats'] = self.make_beat_annotations(downbeatPositions, num_time_points)\n",
        "\n",
        "      if self.features['tempo']:\n",
        "        # get tempo labels\n",
        "        tempo = np.zeros(num_time_points)\n",
        "\n",
        "        # inter-beat-intervals\n",
        "        IBIs = np.squeeze(np.diff(beats))\n",
        "\n",
        "        # create histogram of IBI values\n",
        "        for i in range(len(beatPositions) - 1):\n",
        "          tempo[beatPositions[i]: beatPositions[i + 1]] = IBIs[i]\n",
        "        # fill in ends\n",
        "        tempo[: beatPositions[0]] = IBIs[0]\n",
        "        tempo[beatPositions[-1]: ] = IBIs[-1]\n",
        "\n",
        "        # use histogram to smooth with 15-frame Hamming window\n",
        "        window = np.hamming(15)\n",
        "        smoothTempo = signal.convolve(tempo, window, mode='same')\n",
        "        smoothTempo = smoothTempo / np.sum(window)\n",
        "\n",
        "        # apply quadratic interpolation to smoothed beat values\n",
        "        beatPositions_for_tempo = list(sorted(set(beatPositions)))\n",
        "        smoothed = smoothTempo[beatPositions_for_tempo]\n",
        "        # add values for the start and end positions\n",
        "        smoothed = np.concatenate(([smoothed[0]], smoothed, [smoothed[-1]]))\n",
        "        tempoPositions = np.concatenate(([0], beatPositions_for_tempo, [num_time_points - 1]))\n",
        "\n",
        "        try:\n",
        "          f_interp = interp1d(tempoPositions, smoothed, kind='quadratic')\n",
        "        except ValueError:\n",
        "          print(f\"Beats not annotated consecutively for {path}. Example not included in dataset.\")\n",
        "          continue\n",
        "        tempo = np.arange(num_time_points)\n",
        "        tempo = f_interp(tempo)\n",
        "        \n",
        "\n",
        "        if self.tempo_target_type == 'discrete':\n",
        "          # convert from IBIs (in secs) to BPM\n",
        "          tempo = np.round(np.divide(60.0, tempo)).astype(int)\n",
        "\n",
        "          # map to BPM probability distribution matrix\n",
        "          tempoLabels = np.zeros((self.tempo_range[1] + 2, num_time_points))\n",
        "          tempoHist = [0.25, 0.5, 1, 0.5, 0.25]\n",
        "          skip = False\n",
        "          for j, t in enumerate(tempo):\n",
        "            # check that tempi fall within valid range\n",
        "            if not self.tempo_range[0] <= t <= self.tempo_range[1]:\n",
        "              print(f\"Tempo of {t} out of range for {path}. Example not included in dataset.\")\n",
        "              skip = True\n",
        "              break\n",
        "            else:\n",
        "              tempoLabels[t - 3: t + 2, j] = tempoHist\n",
        "\n",
        "          if skip:\n",
        "            continue\n",
        "\n",
        "          # trim tempo to self.tempo_range\n",
        "          data['tempo'] = tempoLabels[self.tempo_range[0] - 1: self.tempo_range[1], :]\n",
        "\n",
        "        elif self.tempo_target_type == 'continuous': \n",
        "          if self.tempo_range is not None:\n",
        "            # check that tempo is in range\n",
        "              if not (min(tempo) >= self.tempo_range[0] / 60.0) and \\\n",
        "                     (max(tempo) <= self.tempo_range[1] / 60.0):\n",
        "                print(f\"Tempo of {min(tempo)} or {max(tempo)} out of range for {path}. \" \\\n",
        "                      f\"Example not included in dataset.\")\n",
        "                continue\n",
        "          # convert from IBIs (in secs) to BPS\n",
        "          tempo = np.divide(1.0, tempo)\n",
        "          data['tempo'] = tempo\n",
        "\n",
        "        else:\n",
        "          raise RuntimeError(\"Invalid tempo type for self.tempo_target_type.  \" \\\n",
        "                             \"Should be 'continuous' or 'discrete'.  \")\n",
        "\n",
        "      yield path, data\n",
        "\n",
        "\n",
        "  def load_metadata(self):\n",
        "    \"\"\"\n",
        "    Load the metadata json file from the linked folder\n",
        "    \"\"\"\n",
        "    try:\n",
        "      with open(Path(self.get_path(self.name), 'metadata.json')) as json_file:\n",
        "        file = json.load(json_file)\n",
        "    except FileNotFoundError:\n",
        "      print(\"Metadata file could not be loaded\")\n",
        "      file = {}\n",
        "    return file\n",
        "\n",
        "\n",
        "  def folder_exists(self, name):\n",
        "    return os.path.exists(self.get_path(name))\n",
        "\n",
        "\n",
        "  def get_path(self, name):\n",
        "    \"\"\"\n",
        "    Return a string to the current folder directory\n",
        "    \"\"\"\n",
        "    return os.path.join(self.base, name)\n",
        "\n",
        "\n",
        "  def reset_features(self, features=None):\n",
        "    \"\"\"\n",
        "    features (list): \n",
        "      if None: change all features to 'False'\n",
        "      if given: change these features to 'True' and all others to 'False'\n",
        "    \"\"\"\n",
        "    for feature, val in self.features.items():\n",
        "      val = False\n",
        "    if features is not None:\n",
        "      for f in features:\n",
        "        self.features[f] = True\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def make_beat_annotations(beatPos, length):\n",
        "    # create beat labels (and 0.5 either side), given a list of beat indices\n",
        "    beatLabels = np.zeros(length)\n",
        "    for t in beatPos:\n",
        "      beatLabels[t - 1] = 0.5\n",
        "      beatLabels[t] = 1\n",
        "      beatLabels[t + 1] = 0.5\n",
        "    return beatLabels\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpa48RUbvKkg"
      },
      "outputs": [],
      "source": [
        "# split metadata dictionary into train, validation and test\n",
        "# Note: split metadata by files so samples from the same file are not in both training and test data\n",
        "\n",
        "def split_dataset(metadata, \n",
        "                  max_examples=None,\n",
        "                  test_ratio=0.2, \n",
        "                  validation_ratio=0, \n",
        "                  seed=1):\n",
        "  \"\"\"\n",
        "  Splits metadata into train [validation, ] and test dictionaries\n",
        "\n",
        "  metadata: {'directory': str, \n",
        "             'names': list, \n",
        "             'original_names': str,\n",
        "             'lengths': list, \n",
        "             'features': list, \n",
        "             'sample_rate': int, \n",
        "             'tempo_range': [int, int], \n",
        "             'note_range': [int, int), \n",
        "             'rests': bool}\n",
        "  \"\"\"\n",
        "  def make_split(data, ratio):\n",
        "    length = min(len(data['names']), max_examples) if max_examples else len(data['names'])\n",
        "    train_size = int((1 - ratio) * length)\n",
        "    test_size = length - train_size  \n",
        "\n",
        "    indices = np.arange(length)\n",
        "\n",
        "    train_indices, test_indices = random_split(indices, [train_size, test_size], \n",
        "                                           generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "    train_names, test_names, train_lengths, test_lengths = [], [], [], []\n",
        "    for index in train_indices:\n",
        "      train_names.append(data['names'][index])\n",
        "      train_lengths.append(data['lengths'][index])\n",
        "    for index in test_indices:\n",
        "      test_names.append(data['names'][index])\n",
        "      test_lengths.append(data['lengths'][index])\n",
        "\n",
        "    train = copy.deepcopy(data)\n",
        "    test = copy.deepcopy(data)\n",
        "\n",
        "    train['names'] = train_names\n",
        "    train['lengths'] = train_lengths\n",
        "    test['names'] = test_names\n",
        "    test['lengths'] = test_lengths\n",
        "\n",
        "    return train, test\n",
        "\n",
        "  metatrain, metatest = make_split(metadata, test_ratio)\n",
        "\n",
        "  if not validation_ratio:\n",
        "    return metatrain, None, metatest\n",
        "  \n",
        "  metatrain, metavalidation = make_split(metatrain, validation_ratio)\n",
        "\n",
        "  return metatrain, metavalidation, metatest\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L3iyrWPmWNX"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "# https://github.com/cheriell/ICASSP2021-A2S/blob/main/audio2score/data/prtransdataset.py\n",
        "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "\n",
        "class myDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Abstraction of data file system.\n",
        "\n",
        "  Provides pitch and time augmentation of data for each request.\n",
        "  Randomly selects a sample from within a give window.\n",
        "\n",
        "  Parameters:\n",
        "    metadata: Dictionary describing data files:\n",
        "              {'directory': str, \n",
        "              'names': list, \n",
        "              'original_names': str,\n",
        "              'lengths': list, \n",
        "              'features': list, \n",
        "              'sample_rate': int, \n",
        "              'tempo_range': [int, int], \n",
        "              'note_range': [int, int), \n",
        "              'rests': bool}\n",
        "\n",
        "    sample_length: sumber of time samples per example\n",
        "\n",
        "    pitch_aug_range: range of number of semitones for random pitch augmentation: tuple(int, int)\n",
        "\n",
        "    time_aug_range: range of ratios for random time warping, \n",
        "      ratios should be within in the range (0.5, 2): tuple(float, float).\n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               metadata: dict, \n",
        "               sample_length: int, \n",
        "               pitch_aug_range=None, \n",
        "               time_aug_range=None):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.metadata = metadata\n",
        "\n",
        "    self.sample_length = sample_length\n",
        "    self.pitch_aug_range = pitch_aug_range\n",
        "    self.time_aug_range = time_aug_range\n",
        "\n",
        "    self.index_dict = self.indices_to_positions()\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.index_dict)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index: int) -> dict:\n",
        "    \"\"\"\n",
        "    Returns a sample from the dataset corresponding to the given 'index'.  \n",
        "    Beginning time point of the sample is selected randomly from the range between successive hops, \n",
        "      according to a uniform distribution.  \n",
        "    Data is augmented by time or pitch adjustment, according to the respective attributes.  \n",
        "    \"\"\"\n",
        "    # get the data from file\n",
        "    f_name = self.index_dict[index][0]\n",
        "    filename = os.path.join(self.metadata['directory'], f_name)\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    # load file\n",
        "    with h5py.File(filename, \"r\") as hf:\n",
        "      data['length'] = hf['length'][()]\n",
        "      if 'pianoroll' in hf:\n",
        "        data['pianoroll'] = hf['pianoroll'][:, :]\n",
        "      if 'beats' in hf:\n",
        "        data['beats'] = hf['beats'][:]\n",
        "      if 'downbeats' in hf:\n",
        "        data['downbeats'] = hf['downbeats'][:]\n",
        "      if 'tempo' in hf:\n",
        "        data['tempo'] = hf['tempo'][:]\n",
        "\n",
        "    # with open(filename, 'rb') as f:\n",
        "    #   data = load(f)\n",
        "\n",
        "    # check\n",
        "    if data['pianoroll'].shape[1] != self.index_dict[index][2]:\n",
        "      raise RuntimeError(f\"file {filename} incorrect length: \\n\" \\\n",
        "        f\"pianoroll length: {data['pianoroll'].shape[1]}, dictionary length: {self.index_dict[index][2]}\")\n",
        "\n",
        "    # start index for slicing track and labels\n",
        "    # start index is slected randomly in range between hop start points\n",
        "    start = random.randint(*self.index_dict[index][1])\n",
        "\n",
        "    # set augmentation parameters\n",
        "    # pitch augmentation\n",
        "    shift = 0\n",
        "    if self.pitch_aug_range is not None:\n",
        "      # choose augmentation parameter\n",
        "      shift = random.randint(*self.pitch_aug_range)\n",
        "    # time augmentation\n",
        "    num_samples = self.sample_length\n",
        "    if self.time_aug_range is not None:\n",
        "      num_samples, start = self.augment_time(start, index)\n",
        "\n",
        "    # initialise example dictionary\n",
        "    example = {}\n",
        "    # iterate through features and add to dictionary\n",
        "    for feature in self.metadata['features']:\n",
        "      item = data[feature]\n",
        "\n",
        "      # print(f\"{feature} shape: {item.shape}\")\n",
        "\n",
        "      item = np.squeeze(item)\n",
        "      if len(item.shape) == 1:\n",
        "        item = np.expand_dims(item, 0)\n",
        "\n",
        "      item = item[:, start: start + num_samples]\n",
        "\n",
        "      # print(f\"{feature} shape: {item.shape}\")\n",
        "\n",
        "      # resample if it was time-augmented\n",
        "      if item.shape[1] != self.sample_length:\n",
        "        item = signal.resample(item, self.sample_length, axis=1)\n",
        "\n",
        "      # apply pitch augmentation\n",
        "      if shift and (feature in ['pianoroll', 'noteroll', 'velocityroll', 'key_signatures']):\n",
        "        item = self.augment_pitch(item, feature, shift)\n",
        "      \n",
        "      # convert to Tensor\n",
        "      item = torch.from_numpy(item).float()\n",
        "      item = torch.squeeze(item)\n",
        "\n",
        "      # add to example dictionary\n",
        "      example[feature] = item\n",
        "\n",
        "      # add name information\n",
        "      example['name'] = f_name\n",
        "      example['original_name'] = self.index_dict[index][3]\n",
        "\n",
        "      # start sample information for testing\n",
        "      example['start'] = start\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "  def indices_to_positions(self):\n",
        "    \"\"\"\n",
        "    Creates a lookup dict for each index as {index: (filename, min_pos, max_pos)}\n",
        "    The min_pos and max_pos define an interval in which the starting position of \n",
        "      the example can be taken.  \n",
        "    \"\"\"\n",
        "    positions = {}\n",
        "    index = 0\n",
        "    for i, name in enumerate(self.metadata['names']):\n",
        "      length = self.metadata['lengths'][i]\n",
        "      original_name = self.metadata['original_names'][i]\n",
        "\n",
        "      # check that piece is long enough\n",
        "      if length < self.sample_length:\n",
        "        continue\n",
        "\n",
        "      # number of examples in this file\n",
        "      num = int(length / self.sample_length)\n",
        "\n",
        "      # create an entry for each example\n",
        "      for hop in range(num):\n",
        "        start = hop * self.sample_length\n",
        "\n",
        "        # find last possible start point, avoiding the slice going out of range\n",
        "        end = min(start + self.sample_length - 1, length - self.sample_length)\n",
        "\n",
        "        positions[index] = (name, (start, end), length, original_name)\n",
        "\n",
        "        index += 1\n",
        "\n",
        "    return positions\n",
        "\n",
        "\n",
        "  def augment_time(self, start, index):\n",
        "    \"\"\"\n",
        "    Augment time by choosing a different number of time samples to take from the database, \n",
        "      and this will then be resampled to the correct number of points.  \n",
        "    \"\"\"\n",
        "    # choose augmentation parameter\n",
        "    warp = random.uniform(*self.time_aug_range)\n",
        "    # new sample length required\n",
        "    num_samples = self.sample_length\n",
        "    num_samples = (1 / warp) * num_samples\n",
        "    # move start if necessary\n",
        "    start = min(start, self.index_dict[index][2] - num_samples)\n",
        "    # shorten length if necessary\n",
        "    if start < 0:\n",
        "      num_samples = num_samples + start\n",
        "      start = 0\n",
        "    \n",
        "    return int(num_samples), int(start)\n",
        "\n",
        "\n",
        "  def augment_pitch(self, item, feature, shift):\n",
        "    \n",
        "    if shift == 0:\n",
        "      return item\n",
        "\n",
        "    # pianoroll\n",
        "    if feature in ['pianoroll', 'noteroll', 'velocityroll']:\n",
        "      padding = np.zeros((np.abs(shift), item.shape[1]))\n",
        "      if self.metadata['rests']:\n",
        "        rests = item[-1, :]\n",
        "        item = item[:-1, :]\n",
        "      if shift > 0:\n",
        "        item = item[:-shift, :]\n",
        "        item = np.concatenate((padding, item))\n",
        "      else:\n",
        "        item = item[-shift: , :]\n",
        "        item = np.concatenate((item, padding))\n",
        "      if self.metadata['rests']:\n",
        "        item = np.concatenate((item, rests))\n",
        "        \n",
        "    # key_signatures (potential extension)\n",
        "\n",
        "    return item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw0KVPrUORkX"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvBNN1a4iLXu"
      },
      "source": [
        "## General Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "remF9SkvxPpS"
      },
      "outputs": [],
      "source": [
        "# To remove padding added for dilated convolution\n",
        "# https://github.com/locuslab/TCN/blob/master/TCN/tcn.py [Bai]\n",
        "\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "  \"\"\"\n",
        "  Removes padding\n",
        "  \"\"\"\n",
        "  def __init__(self, padding):\n",
        "    super(Chomp1d, self).__init__()\n",
        "\n",
        "    self.chomp = padding\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x[:, :, : -self.chomp].contiguous()\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPghYuaOyv74"
      },
      "outputs": [],
      "source": [
        "# utility function for initialising layers\n",
        "\n",
        "def init_layer(layer):\n",
        "    \"\"\"Initialize a Linear or Convolutional layer. \n",
        "    Ref: He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing \n",
        "    human-level performance on imagenet classification.\" Proceedings of the \n",
        "    IEEE international conference on computer vision. 2015.\n",
        "    \"\"\"\n",
        "    if layer.weight.ndimension() == 4:\n",
        "        (n_out, n_in, height, width) = layer.weight.size()\n",
        "        n = n_in * height * width\n",
        "    \n",
        "    elif layer.weight.ndimension() == 3:\n",
        "        (n_out, n_in, width) = layer.weight.size()\n",
        "        n = n_in * width\n",
        "\n",
        "    elif layer.weight.ndimension() == 2:\n",
        "        (n_out, n) = layer.weight.size()\n",
        "\n",
        "    std = math.sqrt(2. / n)\n",
        "    scale = std * math.sqrt(3.)\n",
        "    layer.weight.data.uniform_(-scale, scale)\n",
        "\n",
        "    if layer.bias is not None:\n",
        "        layer.bias.data.fill_(0.)\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrd8fR9ca8zP"
      },
      "outputs": [],
      "source": [
        "# Convolution blocks for input\n",
        "\n",
        "class ConvBlock2D(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "               in_channels, \n",
        "               out_channels, \n",
        "               kernel, \n",
        "               padding, \n",
        "               maxpool=None, \n",
        "               activation=nn.ELU, \n",
        "               dropout=0.1):\n",
        "    \n",
        "    super(ConvBlock2D, self).__init__()\n",
        "\n",
        "    self.maxpool = maxpool\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel, padding=padding)\n",
        "    self.act = activation()\n",
        "    if dropout:\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "    if maxpool is not None:\n",
        "      self.pool = nn.MaxPool2d(maxpool) \n",
        "    \n",
        "    self.init_weights()\n",
        "\n",
        "\n",
        "  def init_weights(self):\n",
        "    init_layer(self.conv)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    if self.dropout:\n",
        "      out = self.dropout(out)\n",
        "    if self.maxpool is not None:\n",
        "      out = self.pool(out)\n",
        "    out = self.act(out)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAmCMTvh9phM"
      },
      "outputs": [],
      "source": [
        "#temporal convolution modules\n",
        "# https://www.kaggle.com/code/ceshine/pytorch-temporal-convolutional-networks/script\n",
        "# https://github.com/locuslab/TCN/blob/master/TCN/tcn.py [Bai]\n",
        "# https://github.com/ben-hayes/beat-tracking-tcn/blob/master/beat_tracking_tcn/datasets/ballroom_dataset.py\n",
        "# https://github.com/CPJKU/madmom/blob/main/madmom/ml/nn/layers.py [Boeck]\n",
        "\n",
        "\n",
        "class DilatedSection(nn.Module):\n",
        "  \"\"\"\n",
        "  Dilated convolution sequential module\n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               num_inputs,\n",
        "               num_outputs, \n",
        "               kernel_size, \n",
        "               dilation, \n",
        "               padding, \n",
        "               activation, \n",
        "               dropout=0.1, \n",
        "               weightnorm=True, \n",
        "               causal=False):\n",
        "    \"\"\"\n",
        "    Note: padding must be in form (left, right)\n",
        "    \"\"\"\n",
        "    super(DilatedSection, self).__init__()\n",
        "\n",
        "    self.conv = nn.Conv1d(num_inputs, num_outputs, kernel_size=kernel_size, padding=padding, \n",
        "                          dilation=dilation)\n",
        "    if weightnorm:\n",
        "      self.conv = weight_norm(self.conv)\n",
        "    \n",
        "    self.chomp = None\n",
        "    self.causal = causal\n",
        "    if causal:\n",
        "      self.chomp = Chomp1d(padding)\n",
        "\n",
        "    self.act = activation()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "\n",
        "  def init_weights(self):\n",
        "    init_layer(self.conv)\n",
        "  \n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # assert x.shape == (4, 16, 4349), f\"x.shape: {x.shape}\"\n",
        "\n",
        "    out = self.conv(x)\n",
        "    if self.causal:\n",
        "      out = self.chomp(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.act(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  TCN block\n",
        "\n",
        "  Option for skip connection after dilated convolutions\n",
        "\n",
        "  Sequence of n dilations summed with residual\n",
        "\n",
        "  Option for 1x1 convolution (res_conv) when there is no downsampling of layers\n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               num_inputs, \n",
        "               num_outputs, \n",
        "               kernel_size, \n",
        "               dilation, \n",
        "               padding, \n",
        "               dilated_act=nn.ReLU, \n",
        "               block_act=nn.ReLU, \n",
        "               dropout=0.1, \n",
        "               weightnorm=True, \n",
        "               skip=False,\n",
        "               res_conv=True, \n",
        "               causal=False):\n",
        "    \"\"\"\n",
        "    Note: kernel_size, stride, dilation and padding must all have the same length\n",
        "    \"\"\"\n",
        "\n",
        "    super(TemporalBlock, self).__init__()\n",
        "\n",
        "    self.skip = skip\n",
        "    self.res_conv = res_conv\n",
        "    # force 1D convolution True if downsampling required\n",
        "    if num_inputs != num_outputs:\n",
        "      self.res_conv = True\n",
        "\n",
        "    # temporal convolutions ections\n",
        "    dilated_layers = []\n",
        "    for i in range(len(kernel_size)):\n",
        "      in_channels = num_inputs if i == 0 else num_outputs[i - 1]\n",
        "      dilated_layers += [DilatedSection(in_channels, num_outputs, kernel_size[i],\n",
        "                                       dilation[i], padding[i], dilated_act, \n",
        "                                       dropout=dropout, weightnorm=weightnorm, causal=causal)]\n",
        "    self.dilated = nn.Sequential(*dilated_layers)\n",
        "    \n",
        "    # optional skip connection output after dilated layers\n",
        "    if skip:\n",
        "      self.conv1d_skip = nn.Conv1d(num_outputs, num_outputs, 1)\n",
        "      self.dilated = nn.Sequential(self.dilated, self.conv1d_skip)\n",
        "    \n",
        "    # optional 1D convolution for internal skip connection\n",
        "    # also provides downsampling if number of channels changes\n",
        "    self.downsample = nn.Conv1d(num_inputs, num_outputs, 1) if self.res_conv else None\n",
        "\n",
        "    self.block_act = block_act()\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "\n",
        "  def init_weights(self):\n",
        "    if self.skip:\n",
        "      init_layer(self.conv1d_skip)\n",
        "    init_layer(self.downsample)\n",
        "  \n",
        "  \n",
        "  def forward(self, x):\n",
        "    \n",
        "    out = self.dilated(x)\n",
        "    res = x if self.downsample is None else self.downsample(x)\n",
        "    skip = out if self.skip else None\n",
        "\n",
        "    out = self.block_act(out + res)\n",
        "    return out, skip\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZNTOAnIT9VX"
      },
      "source": [
        "## Sections\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "519rQs4wbMRY"
      },
      "outputs": [],
      "source": [
        "# convolutional section mirroring application for audio in Bock(2019)\n",
        "\n",
        "class Conv_original(nn.Module):\n",
        "  \n",
        "  def __init__(self, \n",
        "               in_channels=1, \n",
        "               conv_channels=(16, 16, 16), \n",
        "               kernels=((3, 3), (3, 3), (8, 1)), \n",
        "               padding=((0, 1), (0, 1), (0, 0)), \n",
        "               maxpool=((3, 1), (3, 1), None), \n",
        "               dropout=0.1, \n",
        "               act=nn.ELU):\n",
        "    \n",
        "    super(Conv_original, self).__init__()\n",
        "\n",
        "    # convolution sections\n",
        "    layers = []\n",
        "    for i in range(len(conv_channels)):\n",
        "      input = in_channels if i == 0 else conv_channels[i - 1]\n",
        "      layers += [ConvBlock2D(input, conv_channels[i], kernels[i], padding=padding[i], \n",
        "                             maxpool=maxpool[i], activation=act, dropout=dropout)]\n",
        "    \n",
        "    self.net = nn.Sequential(*layers)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    return self.net(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hge87hRHSWFW"
      },
      "outputs": [],
      "source": [
        "# Original TCN section\n",
        "\n",
        "class TCN_original(nn.Module):\n",
        "  \"\"\"\n",
        "  Full temporal convolutional section\n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               num_inputs=16, \n",
        "               num_channels=None, \n",
        "               kernel_sizes=None, \n",
        "               dilations=None, \n",
        "               dilated_act=nn.ELU, \n",
        "               block_act=nn.ELU, \n",
        "               dropout=0.1, \n",
        "               weightnorm=True, \n",
        "               skip=False, \n",
        "               res_conv=True, \n",
        "               causal=False):\n",
        "    \"\"\"\n",
        "    Note: num_channels, kernel_sizes and dilations must all have the same length\n",
        "    \"\"\"\n",
        "    super(TCN_original, self).__init__()\n",
        "\n",
        "    if num_channels is None:\n",
        "      num_channels=[16] * 10\n",
        "    if kernel_sizes is None:\n",
        "      kernel_sizes=[[5]] * 10\n",
        "    if dilations is None:\n",
        "      dilations = [[2 ** n] for n in range(10)]\n",
        "\n",
        "    self.layers = []\n",
        "    num_layers = len(num_channels)\n",
        "    for i in range(num_layers):\n",
        "      # number of input channels\n",
        "      in_channels = num_channels[i - 1] if i > 1 else num_inputs\n",
        "\n",
        "      sections = len(dilations[i])\n",
        "\n",
        "      # calculate padding sizes\n",
        "      total_pad = [(kernel_sizes[i][n] - 1) * dilations[i][n] for n in range(sections)]\n",
        "      if causal: \n",
        "        padding = [p for p in total_pad]\n",
        "      else:\n",
        "        padding = [int(p / 2) for p in total_pad]\n",
        "\n",
        "      self.layers += [TemporalBlock(in_channels, num_channels[i], kernel_sizes[i], \n",
        "                                    dilations[i], padding, dilated_act=dilated_act, dropout=dropout, \n",
        "                                    weightnorm=weightnorm, skip=skip, res_conv=res_conv, causal=causal)]\n",
        "    # self.net = nn.Sequential(*layers)\n",
        "      self.net = nn.ModuleList(self.layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out = x\n",
        "    skips = []\n",
        "    for layer in self.net:\n",
        "      out, skip = layer(out)\n",
        "      skips.append(skip)\n",
        "      \n",
        "    return out, skips\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wvv18d5DPIB"
      },
      "outputs": [],
      "source": [
        "# Convolutional block for tempo network\n",
        "\n",
        "class Tempo_conv_block(nn.Module):\n",
        "  \"\"\"\n",
        "  Full temporal convolutional section\n",
        "  \"\"\"\n",
        "  def __init__(self, \n",
        "               num_inputs=16, \n",
        "               num_channels=(16, 16, 16, 16), \n",
        "               kernel_sizes=(3, 3, 3, 3), \n",
        "               dilations=(1, 1, 1, 1), \n",
        "               padding=(1, 1, 1, 1), \n",
        "               maxpool=(2, 2, 2, 1),\n",
        "               act=nn.ELU, \n",
        "               dropout=0.1, \n",
        "               weightnorm=True):\n",
        "    \"\"\"\n",
        "    Note: num_channels, kernel_sizes and dilations must all have the same length\n",
        "    \"\"\"\n",
        "    super(Tempo_conv_block, self).__init__()\n",
        "\n",
        "    layers = []\n",
        "    num_layers = len(num_channels)\n",
        "    for i in range(num_layers):\n",
        "      in_channels = num_inputs if i == 0 else num_channels[i - 1]\n",
        "      layers += [DilatedSection(num_inputs, num_channels[i], kernel_sizes[i], dilations[i], \n",
        "                                padding[i], act, dropout=dropout, weightnorm=weightnorm)]\n",
        "\n",
        "      layers += [nn.BatchNorm1d(in_channels), \n",
        "                 nn.Conv1d(in_channels, num_channels[i], kernel_sizes[i], padding=padding[i], \n",
        "                           dilation=dilations[i]), \n",
        "                 nn.MaxPool1d(maxpool[i]), \n",
        "                 nn.Dropout(dropout),\n",
        "                 act()]\n",
        "\n",
        "    self.net = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out = self.net(x)\n",
        "      \n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7byyw5viQIq"
      },
      "source": [
        "## Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8P0aiTGiSQj"
      },
      "outputs": [],
      "source": [
        "# TCN beat tracker\n",
        "# https://github.com/CPJKU/madmom/tree/main/madmom\n",
        "\n",
        "\n",
        "class TCNbeatTracker(nn.Module):\n",
        "  \n",
        "  def __init__(self, \n",
        "               downbeats=False, \n",
        "               conv=Conv_original, \n",
        "               conv_channels=(16, 16, 16), \n",
        "               conv_kwargs={}, \n",
        "               tcn=TCN_original, \n",
        "               tcn_args=[], \n",
        "               tcn_kwargs=None, \n",
        "               dropout=0.1):\n",
        "    \n",
        "    super(TCNbeatTracker, self).__init__()\n",
        "\n",
        "    if tcn_kwargs is None:\n",
        "      tcn_kwargs={'num_channels': [16]*10}\n",
        "\n",
        "    # convolution feature extractor\n",
        "    # if conv == \"original\":\n",
        "      # self.conv = Conv_original(conv_channels=conv_channels, **conv_kwargs)  \n",
        "    self.conv = conv(conv_channels=conv_channels, **conv_kwargs)  \n",
        "\n",
        "    # TCN section with a wide receptive field\n",
        "    self.tcn = tcn(conv_channels[-1], *tcn_args, dropout=dropout, **tcn_kwargs)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Time-distributed fully-connected layer\n",
        "    self.beat_time_dist = nn.Conv1d(tcn_kwargs['num_channels'][-1], 1, 1)\n",
        "    # self.beat_sigmoid = nn.Sigmoid()\n",
        "\n",
        "    self.downbeats = downbeats\n",
        "    if downbeats:\n",
        "      # Time-distributed fully-connected layer\n",
        "      self.downbeat_time_dist = nn.Conv1d(tcn_kwargs['num_channels'][-1], 1, 1)\n",
        "      # self.downbeat_sigmoid = nn.Sigmoid()\n",
        "\n",
        "    \n",
        "  def forward(self, x):\n",
        "\n",
        "    (_, notes, time_len) = x.shape\n",
        "    # reshape the input into 4D format (batch_size, channels, notes, time_len)\n",
        "    x = x.view(-1, 1, notes, time_len)\n",
        "\n",
        "    out = self.conv(x)\n",
        "\n",
        "    out = out.view(-1, out.shape[1], out.shape[3])\n",
        "    out, skips = self.tcn(out)\n",
        "\n",
        "    out = self.dropout(out)\n",
        "\n",
        "    beats = self.beat_time_dist(out)\n",
        "    # beats = self.beat_sigmoid(beats)\n",
        "    beats = torch.squeeze(beats)\n",
        "\n",
        "    downbeats = None\n",
        "    if self.downbeats: \n",
        "      downbeats = self.downbeat_time_dist(out)\n",
        "      # downbeats = self.downbeat_sigmoid(downbeats)\n",
        "      downbeats = torch.squeeze(downbeats)\n",
        "\n",
        "    return beats, downbeats, None\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def loss_functions():\n",
        "\n",
        "    def loss(beats, beat_labels, downbeats=None, downbeat_labels=None, downbeat_weight=0.5, \n",
        "             tempo=None, tempo_labels=None, tempo_weight=0, pos_weight=None):\n",
        "      beat_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(beats, beat_labels)\n",
        "      downbeat_loss = 0\n",
        "      if downbeats:\n",
        "        downbeat_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight*2)(downbeats, downbeat_labels)\n",
        "      return beat_loss + downbeat_weight * downbeat_loss \n",
        "\n",
        "    def beat_loss(beats, beat_labels, pos_weight=None):\n",
        "      return nn.BCEWithLogitsLoss(pos_weight=pos_weight)(beats, beat_labels)\n",
        "\n",
        "    def downbeat_loss(downbeats, downbeat_labels, pos_weight=None):\n",
        "      return nn.BCEWithLogitsLoss(pos_weight=pos_weight*2)(downbeats, downbeat_labels)\n",
        "\n",
        "    return {'loss': loss, 'beat_loss': beat_loss, 'downbeat_loss': downbeat_loss}\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kz_HHnjCUoV"
      },
      "outputs": [],
      "source": [
        "# CNN tempo classifier\n",
        "\n",
        "class CNNtempoTracker(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "               downbeats=False, \n",
        "               conv=Conv_original, \n",
        "               conv_kwargs={'conv_channels': (16, 16, 16)}, \n",
        "               conv2=Tempo_conv_block, \n",
        "               conv2_kwargs={'num_channels': (16, 16, 16, 16)}, \n",
        "               tempo_target_type='continuous'):\n",
        "    \n",
        "    super(CNNtempoTracker, self).__init__()\n",
        "\n",
        "    # convolution feature extractor\n",
        "    self.conv = conv(**conv_kwargs)  \n",
        "    \n",
        "    # TCN section with a wide receptive field\n",
        "    self.conv2 = conv2(conv_kwargs['conv_channels'][-1], **conv2_kwargs)\n",
        "\n",
        "    if tempo_target_type == 'continuous':\n",
        "      self.time_dist = nn.Conv1d(conv2_kwargs['num_channels'][-1], 1, 1)\n",
        "\n",
        "    elif tempo_target_type == 'discrete':\n",
        "      pass\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "\n",
        "  def init_weights(self):\n",
        "\n",
        "    init_layer(self.time_dist)\n",
        "  \n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    (_, notes, time_len) = x.shape\n",
        "    # reshape the input into 4D format (batch_size, channels, notes, time_len)\n",
        "    x = x.view(-1, 1, notes, time_len)\n",
        "\n",
        "    out = self.conv(x)\n",
        "    out = out.view(-1, out.shape[1], out.shape[3])\n",
        "\n",
        "    out = self.conv2(out)\n",
        "\n",
        "    out = self.time_dist(out)\n",
        "\n",
        "    # # use interpolation to get the correct time reolution for target comparison\n",
        "    # out = nn.functional.interpolate(out, size=(time_len), mode='nearest')\n",
        "\n",
        "    return None, None, out\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def loss_functions():\n",
        "    \n",
        "    def loss(beats=None, beat_labels=None, dpwnbeats=None, downbeat_labels=None, downbeat_weight=0, \n",
        "             tempo=None, tempo_labels=None, tempo_weight=0, pos_weight=None):\n",
        "      return nn.MSELoss()(tempo, tempo_labels)\n",
        "\n",
        "    return {'loss': loss}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adNJIgYKK1wf"
      },
      "outputs": [],
      "source": [
        "# Multitask TCN for joint beat and tempo tracking\n",
        "\n",
        "class TCN_Multitask(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "               downbeats=False, \n",
        "               conv=Conv_original, \n",
        "               conv_kwargs={'conv_channels': (16, 16, 16)}, \n",
        "               tcn=TCN_original, \n",
        "               tcn_kwargs={'num_channels': [16] * 10, 'skip': True},\n",
        "               meanpool_size=8, \n",
        "               dropout=0.1, \n",
        "               tempo_dropout=0.5):\n",
        "    \n",
        "    super(TCN_Multitask, self).__init__()\n",
        "\n",
        "    # convolution feature extractor\n",
        "    self.conv = conv(**conv_kwargs)  \n",
        "    \n",
        "    # shared TCN section\n",
        "    self.tcn = tcn(conv_kwargs['conv_channels'][-1], **tcn_kwargs)\n",
        "\n",
        "    # beat tracking branch\n",
        "    self.beat_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Time-distributed fully-connected layer\n",
        "    self.beat_time_dist = nn.Conv1d(tcn_kwargs['num_channels'][-1], 1, 1)\n",
        "    # self.beat_sigmoid = nn.Sigmoid()\n",
        "\n",
        "    self.downbeats = downbeats\n",
        "    if downbeats:\n",
        "      # Time-distributed fully-connected layer\n",
        "      self.downbeat_time_dist = nn.Conv1d(tcn_kwargs['num_channels'][-1], 1, 1)\n",
        "      # self.downbeat_sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # tempo branch\n",
        "    self.tempo_meanpool = nn.AvgPool1d(meanpool_size)\n",
        "    self.tempo_dropout = nn.Dropout(tempo_dropout)\n",
        "    self.tempo_time_dist = nn.Conv1d(tcn_kwargs['num_channels'][-1], 1, 1)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    init_layer(self.beat_time_dist)\n",
        "    if self.downbeats:\n",
        "      init_layer(self.downbeat_time_dist)\n",
        "    init_layer(self.tempo_time_dist)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    (_, notes, time_len) = x.shape\n",
        "    # reshape the input into 4D format (batch_size, channels, notes, time_len)\n",
        "    x = x.view(-1, 1, notes, time_len)\n",
        "\n",
        "    out = self.conv(x)\n",
        "    out = out.view(-1, out.shape[1], out.shape[3])\n",
        "\n",
        "    beats, skips = self.tcn(out)\n",
        "    tempo = torch.stack(skips).sum(dim=0)\n",
        "\n",
        "    tempo = self.tempo_meanpool(tempo)\n",
        "    tempo = self.tempo_dropout(tempo)\n",
        "    tempo = self.tempo_time_dist(tempo)\n",
        "\n",
        "    # # use interpolation to get the correct time reolution for target comparison\n",
        "    # tempo = nn.functional.interpolate(tempo, size=(time_len), mode='nearest')\n",
        "\n",
        "    # shared dropout\n",
        "    beats = self.beat_dropout(beats)\n",
        "\n",
        "    # separate beats dense layer\n",
        "    beat_pred = self.beat_time_dist(beats)\n",
        "    # beat_pred = self.beat_sigmoid(beat_pred)\n",
        "    beat_pred = torch.squeeze(beat_pred)\n",
        "\n",
        "    downbeat_pred = None\n",
        "    if self.downbeats: \n",
        "      # separate downbeats dense layer\n",
        "      downbeat_pred = self.downbeat_time_dist(beats)\n",
        "      # downbeat_pred = self.downbeat_sigmoid(downbeat_pred)\n",
        "      downbeat_pred = torch.squeeze(downbeat_pred)\n",
        "\n",
        "    return beat_pred, downbeat_pred, tempo\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def loss_functions():\n",
        "    \n",
        "    def loss(beats=None, beat_labels=None, downbeats=None, downbeat_labels=None, downbeat_weight=0.5, \n",
        "             tempo=None, tempo_labels=None, tempo_weight=0.00025, pos_weight=None):\n",
        "      beat_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)(beats, beat_labels)\n",
        "      downbeat_loss = 0\n",
        "      if downbeats:\n",
        "        downbeat_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight*2)(downbeats, downbeat_labels)\n",
        "      tempo_loss = nn.MSELoss()(tempo, tempo_labels)\n",
        "      return beat_loss + downbeat_weight * downbeat_loss + tempo_weight * tempo_loss\n",
        "\n",
        "    def beat_loss(beats, beat_labels, pos_weight=None):\n",
        "      return nn.BCEWithLogitsLoss(pos_weight=pos_weight)(beats, beat_labels)\n",
        "\n",
        "    def downbeat_loss(downbeats, downbeat_labels, pos_weight=None):\n",
        "      return nn.BCEWithLogitsLoss(pos_weight=pos_weight*2)(downbeats, downbeat_labels)\n",
        "    \n",
        "    def tempo_loss(tempo, tempo_labels, pos_weight=None):\n",
        "      return nn.MSELoss()(tempo, tempo_labels)\n",
        "\n",
        "    return {'loss': loss, 'beat_loss': beat_loss, 'downbeat_loss': downbeat_loss, \n",
        "            'tempo_loss': tempo_loss}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxS4xQdt2EVu"
      },
      "source": [
        "## Network for training loss weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiTN4fqu2HMj"
      },
      "outputs": [],
      "source": [
        "class MultiTaskLoss(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "               device,\n",
        "               beats_loss=nn.BCELoss,\n",
        "               downbeats_loss=None, \n",
        "               tempo_loss=None, \n",
        "               weights_init=None, \n",
        "               beat_manual_weight=1,\n",
        "               downbeat_manual_weight=1, \n",
        "               tempo_manual_weight=1, \n",
        "               beat_pos_weight=None, \n",
        "               reg_factor_ind=1, \n",
        "               reg_factor_sum=1):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    pos_weight = torch.tensor(beat_pos_weight, requires_grad=False)\n",
        "\n",
        "    self.beats_loss = None if beats_loss is None else beats_loss(pos_weight=pos_weight)\n",
        "    self.downbeats_loss = None if downbeats_loss is None else downbeats_loss(pos_weight=pos_weight*2)\n",
        "    self.tempo_loss = None if tempo_loss is None else tempo_loss()\n",
        "\n",
        "    if weights_init is None:\n",
        "      num_weights = 0\n",
        "      for loss in [beats_loss, downbeats_loss, tempo_loss]:\n",
        "        if loss is not None:\n",
        "          num_weights += 1\n",
        "\n",
        "      self.weights = nn.Parameter(torch.ones(num_weights))\n",
        "\n",
        "    else: \n",
        "      self.weights = nn.Parameter(weights_init)\n",
        "\n",
        "    manual_weights = [beat_manual_weight]\n",
        "    if downbeats_loss is not None:\n",
        "      manual_weights.append(downbeat_manual_weight)\n",
        "    if tempo_loss is not None:\n",
        "      manual_weights.append(tempo_manual_weight) \n",
        "    \n",
        "    # for manual (non-learned) loss weights\n",
        "    self.manual_weights = torch.tensor(manual_weights).to(device)\n",
        "\n",
        "    # factors for regularisation of loss weights\n",
        "    self.reg_factor_ind = reg_factor_ind\n",
        "    self.reg_factor_sum = reg_factor_sum\n",
        "\n",
        "  \n",
        "  def forward(self, \n",
        "              beats_output=None, \n",
        "              beats_labels=None, \n",
        "              downbeats_output=None,\n",
        "              downbeats_labels=None,\n",
        "              tempo_output=None,\n",
        "              tempo_labels=None):\n",
        "    \n",
        "    losses = []\n",
        "\n",
        "    if self.beats_loss is not None:\n",
        "      losses += [self.beats_loss(beats_output, beats_labels)]\n",
        "    if self.downbeats_loss is not None:\n",
        "      losses += [self.downbeats_loss(downbeats_output, downbeats_labels)]\n",
        "    if self.tempo_loss is not None:\n",
        "      losses += [self.tempo_loss(tempo_output, tempo_labels)]\n",
        "\n",
        "    # # create weighted multitask loss\n",
        "    # total_loss = torch.stack(losses) / (self.weights ** 2) * self.manual_weights\n",
        "    # # add regularisation to prevent degenerate solution\n",
        "    # total_loss = total_loss + torch.log(1 + (self.weights ** 2))\n",
        "    # total_loss = total_loss.sum()\n",
        "\n",
        "    # create weighted multitask loss\n",
        "    total_loss = torch.stack(losses) * torch.abs(self.weights) * self.manual_weights\n",
        "    # add regularisation to prevent degenerate solution\n",
        "    total_loss = total_loss + torch.abs(torch.log(self.weights)) * self.reg_factor_ind\n",
        "    total_loss = total_loss.sum() + torch.abs(torch.log(self.weights.sum() / 2.0)) * self.reg_factor_sum\n",
        "\n",
        "    return losses, total_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eShnUKTCtlu"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pnt56YkLP4x"
      },
      "outputs": [],
      "source": [
        "# post-processing\n",
        "\n",
        "def times_from_labels(labels):\n",
        "  \"\"\"\n",
        "  Get binary labels for computing the f1-score\n",
        "  \"\"\"\n",
        "  labels = np.array(labels)\n",
        "  positions = np.where(labels == 1)[0]\n",
        "\n",
        "  return positions / 100.0\n",
        "\n",
        "\n",
        "def peak_picker(beat_prob_scores, sr=100, mode='max_filt', min_bpm=10.0, max_bpm=360.0, \n",
        "                max_filt_length=15, threshold=0.25):\n",
        "  \"\"\"\n",
        "  Post-processing for outputs from a beat tracking network.  \n",
        "  \"\"\"\n",
        "  if mode == 'max_filt':\n",
        "\n",
        "    # local peak pick\n",
        "    max_values = maxFilt(beat_prob_scores, max_filt_length)\n",
        "    beats = np.where(np.array(beat_prob_scores) == max_values, beat_prob_scores, 0)\n",
        "\n",
        "    # apply threshold and set peaks to 1\n",
        "    beats = np.where(beats > threshold, 1, 0)\n",
        "\n",
        "    # change positions to times\n",
        "    beats = times_from_labels(beats)\n",
        "\n",
        "  if mode == 'dbn':\n",
        "    dbn = DBNBeatTrackingProcessor(min_bpm=min_bpm, max_bpm=max_bpm, fps=sr)\n",
        "\n",
        "    beats = dbn(beat_prob_scores)\n",
        "  \n",
        "  return beats\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldma9jZbXBIl"
      },
      "outputs": [],
      "source": [
        "# Graph pianoroll, beat positions or tempo curve\n",
        "\n",
        "def graph(pianoroll=None, \n",
        "          beats=None, \n",
        "          beats_pred=None,\n",
        "          downbeats=None, \n",
        "          downbeats_pred=None, \n",
        "          tempo=None, \n",
        "          tempo_pred=None,\n",
        "          min_tempo=6,\n",
        "          sr=100, \n",
        "          note_range=(0, 88), \n",
        "          time_range=None, \n",
        "          tempo_mode='continuous'):\n",
        "  \"\"\"\n",
        "  Utility function for visualising data.  \n",
        "  \"\"\"\n",
        "  # change time range to samples\n",
        "  if time_range is not None:\n",
        "    time_range = np.multiply(time_range, sr)\n",
        "\n",
        "  # plot piano roll\n",
        "  if pianoroll is not None:\n",
        "    if time_range is not None:\n",
        "      pianoroll = pianoroll[:, slice(*time_range)]\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    display.specshow(pianoroll, hop_length=1, sr=sr, x_axis='time', y_axis='cqt_note',\n",
        "                     fmin=pm.note_number_to_hz(note_range[0]))\n",
        "    plt.show()\n",
        "  \n",
        "  # plot tempo\n",
        "  if tempo is not None:\n",
        "    if tempo_mode == 'discrete':\n",
        "      if time_range is not None:\n",
        "        tempo = tempo[:, slice(*time_range)]\n",
        "      if tempo_pred is not None:\n",
        "        time = np.arange(np.array(tempo_pred.shape[1])) / sr\n",
        "        if time_range is not None:\n",
        "          tempo_pred = tempo_pred[slice(*time_range)]\n",
        "          time = time[slice(*time_range)]\n",
        "        tempo = np.squeeze(np.add(np.argmax(tempo, axis=1), min_tempo))\n",
        "        plt.plot(time, tempo, 'k')\n",
        "        plt.plot(time, tempo_pred, 'ro')\n",
        "      else:\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        display.specshow(tempo, hop_length=1, sr=sr, x_axis='time', y_axis='tempo')\n",
        "      plt.show()\n",
        "    \n",
        "    elif tempo_mode == 'continuous':\n",
        "      plt.figure(figsize=(15, 5))\n",
        "      if time_range is not None:\n",
        "        tempo = tempo[slice(*time_range)]\n",
        "      time = np.arange(len(tempo_pred)) / sr * 8\n",
        "      if time_range is not None:\n",
        "        time = time[slice(*time_range)]\n",
        "      plt.plot(time, tempo, 'k')\n",
        "      if tempo_pred is not None:\n",
        "        if time_range is not None:\n",
        "          tempo_pred = tempo_pred[slice(*time_range)]\n",
        "        plt.plot(time, tempo_pred, 'ro')      \n",
        "      plt.show()      \n",
        "\n",
        "  # plot beats and downbeats\n",
        "  if beats is not None:\n",
        "    if time_range is not None:\n",
        "      beats = beats[slice(*time_range)]\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for beat in beats:\n",
        "      plt.vlines(beats, ymin=0, ymax=0.5, colors='teal', ls='--')\n",
        "    if beats_pred is not None:\n",
        "      if time_range is not None:\n",
        "        beats_pred = beats_pred[slice(*time_range)]\n",
        "      plt.vlines(beats_pred, ymin=0.5, ymax=1.0, colors='navy', ls=':')\n",
        "    if downbeats is not None:\n",
        "      if time_range is not None:\n",
        "        downbeats = downbeats[slice(*time_range)]\n",
        "      plt.vlines(np.nonzero(downbeats)[0] / sr, ymin=0, ymax=0.75, colors='deeppink')\n",
        "      if downbeats_pred is not None:\n",
        "        if time_range is not None:\n",
        "          downbeats_pred = downbeats_pred[slice(*time_range)]\n",
        "        plt.vlines(np.nonzero(downbeats_pred)[0] / sr, ymin=0.25, ymax=1.0, \n",
        "                   colors='darkmagenta', ls=':')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV_7Ixfp_x61"
      },
      "outputs": [],
      "source": [
        "# set up Dataset and DataLoader\n",
        "\n",
        "def get_dataloaders(metadata, \n",
        "                    sample_len=4349,\n",
        "                    batch_size=4, \n",
        "                    test_ratio=0.1, \n",
        "                    val_ratio=0.15, \n",
        "                    max_examples=None, \n",
        "                    seed=1, \n",
        "                    pitch_aug=None, \n",
        "                    time_aug=None):\n",
        "  \n",
        "  # split into train, validate and test sets\n",
        "  metatrain, metavalidate, metatest = split_dataset(metadata, \n",
        "                                                    test_ratio=test_ratio, \n",
        "                                                    validation_ratio=val_ratio, \n",
        "                                                    seed=seed, \n",
        "                                                    max_examples=max_examples)\n",
        "  \n",
        "  # Initialise Dataset\n",
        "  traindata = myDataset(metadata=metatrain, \n",
        "                        sample_length=sample_len, \n",
        "                        pitch_aug_range=pitch_aug, \n",
        "                        time_aug_range=time_aug)\n",
        "  \n",
        "  print(f\"Training data: {len(traindata)} sampling windows\")\n",
        "\n",
        "  # create DataLoader\n",
        "  train_loader = DataLoader(traindata, batch_size=batch_size, num_workers=4, pin_memory=True, \n",
        "                            shuffle=True)\n",
        "\n",
        "  validate_loader = None\n",
        "  if metavalidate is not None:\n",
        "    validatedata = myDataset(metadata=metavalidate, \n",
        "                             sample_length=sample_len, \n",
        "                             pitch_aug_range=pitch_aug, \n",
        "                             time_aug_range=time_aug)\n",
        "    \n",
        "    print(f\"Validation data: {len(validatedata)} sampling windows\")\n",
        "\n",
        "    validate_loader = DataLoader(validatedata, batch_size=batch_size, num_workers=4, pin_memory=True, \n",
        "                                 shuffle=True)\n",
        "\n",
        "  testdata = myDataset(metadata=metatest, \n",
        "                       sample_length=sample_len, \n",
        "                       pitch_aug_range=pitch_aug, \n",
        "                       time_aug_range=time_aug)\n",
        "  \n",
        "  print(f\"Test data: {len(testdata)} sampling windows\")\n",
        "\n",
        "  test_loader = DataLoader(testdata, batch_size=batch_size, num_workers=4, pin_memory=True, \n",
        "                           shuffle=True)\n",
        "\n",
        "  return train_loader, validate_loader, test_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLtCytRY5xva"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "\n",
        "def train(num_epochs, \n",
        "          model, \n",
        "          optimizer,\n",
        "          train_loader, \n",
        "          device, \n",
        "          scheduler=None,\n",
        "          loss_network=None,\n",
        "          loss_optimizer=None,\n",
        "          save_name=None,\n",
        "          log_name=None,\n",
        "          update_frequency=100,\n",
        "          evaluation_frequency=5,\n",
        "          evaluation_loader=None,\n",
        "          beats=True,\n",
        "          downbeats=False,\n",
        "          downbeat_weight=0.5,\n",
        "          tempo=False, \n",
        "          tempo_weight=0.5, \n",
        "          tempo_range=(10, 360), \n",
        "          peak_mode='max_filt', \n",
        "          peak_threshold=0.25,\n",
        "          max_filt_length=7,\n",
        "          beat_pos_weight=None):\n",
        "\n",
        "  # for tracking accuracy for saving the best model\n",
        "  best_validation_loss = 1000.0\n",
        "\n",
        "  # get loss functions\n",
        "  if loss_network is None:\n",
        "    criteria = model.loss_functions()\n",
        "    overall_criterion = criteria['loss']\n",
        "\n",
        "    if beats:\n",
        "      beat_criterion = criteria['beat_loss']\n",
        "    if downbeats:\n",
        "      downbeat_criterion = criteria['downbeat_loss']\n",
        "    if tempo and beats:\n",
        "      tempo_criterion = criteria['tempo_loss']\n",
        "    print(\"Using static loss weights\")\n",
        "  else:\n",
        "    loss_network.train()\n",
        "    loss_network.to(device)\n",
        "    print(\"Using trainable loss network\")\n",
        "\n",
        "  # set positive label loss weightings\n",
        "  pos_weight=None\n",
        "  if beat_pos_weight is not None:\n",
        "    pos_weight = torch.Tensor(beat_pos_weight).to(device)\n",
        "\n",
        "  model.train()\n",
        "  model.to(device)\n",
        "\n",
        "  # for graphing\n",
        "  output_data = {}\n",
        "  output_data['train_losses'] = []\n",
        "  output_data['validation_losses'] = []\n",
        "  if beats:\n",
        "    output_data['beat_f1s'] = []\n",
        "    if loss_network is not None:\n",
        "      output_data['beat loss weight'] = []\n",
        "    if tempo or downbeats:\n",
        "      output_data['beat_train_losses'] = []\n",
        "      output_data['beat_val_losses'] = []\n",
        "  if downbeats:\n",
        "    output_data['downbeat_f1s'] = []\n",
        "    output_data['downbeat_train_losses'] = []\n",
        "    output_data['downbeat_val_losses'] = []\n",
        "    if loss_network is not None:\n",
        "      output_data['downbeat loss weight'] = []\n",
        "  if tempo:\n",
        "    output_data['tempo_RMSE'] = []\n",
        "    if loss_network is not None:\n",
        "      output_data['tempo loss weight'] = []\n",
        "    if beats:\n",
        "      output_data['tempo_train_losses'] = []\n",
        "      output_data['tempo_val_losses'] = []\n",
        "    \n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_beat_loss = 0\n",
        "    running_downbeat_loss = 0\n",
        "    running_tempo_loss = 0\n",
        "\n",
        "    running_batch_loss = 0\n",
        "\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if loss_network is not None:\n",
        "        loss_optimizer.zero_grad()\n",
        "\n",
        "      # get samples and ground truth labels\n",
        "      x = data['pianoroll']\n",
        "      x = x.to(device)\n",
        "\n",
        "      # get labels\n",
        "      beat_labels=None\n",
        "      if beats:\n",
        "        beat_labels = data['beats']\n",
        "        beat_labels = torch.squeeze(beat_labels)\n",
        "        beat_labels = beat_labels.to(device)\n",
        "      downbeat_labels = None\n",
        "      if downbeats:\n",
        "        downbeat_labels = data['downbeast']\n",
        "        torch.squeeze(downbeat_labels)\n",
        "        downbeat_labels = downbeat_labels.to(device)\n",
        "      tempo_labels = None\n",
        "      if tempo:\n",
        "        tempo_labels = data['tempo']\n",
        "        torch.squeeze(tempo_labels)\n",
        "        tempo_labels = tempo_labels\n",
        "      \n",
        "      # get outputs\n",
        "      beats_pred, downbeats_pred, tempo_pred = model(x)\n",
        "\n",
        "      if beats:\n",
        "        beats_pred = torch.squeeze(beats_pred)\n",
        "      if downbeats:\n",
        "        downbeats_pred = torch.squeeze(downbeats_pred)\n",
        "      if tempo:\n",
        "        tempo_pred = torch.squeeze(tempo_pred)\n",
        "      \n",
        "      # downsample tempo labels\n",
        "      if tempo:\n",
        "        tempo_labels = signal.resample(tempo_labels, tempo_pred.shape[1], axis=1)\n",
        "        tempo_labels = torch.Tensor(tempo_labels).to(device)\n",
        "\n",
        "      # # calculate losses\n",
        "      if loss_network is None:\n",
        "        overall_loss = overall_criterion(beats_pred, beat_labels, downbeats_pred, downbeat_labels, \n",
        "                                        downbeat_weight, tempo_pred, tempo_labels, tempo_weight, \n",
        "                                        pos_weight=pos_weight)\n",
        "      else:\n",
        "        losses, overall_loss = loss_network(beats_pred, beat_labels, downbeats_pred, downbeat_labels, \n",
        "                                        tempo_pred, tempo_labels)\n",
        "      \n",
        "      running_loss += overall_loss.item() * (beats_pred.shape[0] if beats else tempo_pred.shape[0])\n",
        "      running_batch_loss += overall_loss.item()\n",
        "\n",
        "      if downbeats:\n",
        "        if loss_network is None:\n",
        "          beat_loss = beat_criterion(beats_pred, beat_labels, pos_weight=pos_weight)\n",
        "          downbeat_loss = downbeat_criterion(downbeats_pred, downbeat_labels,  \n",
        "                                             pos_weight=pos_weight)\n",
        "        else: \n",
        "          beat_loss = losses[0].detach()\n",
        "          downbeat_loss = losses[1].detach()       \n",
        "        running_beat_loss += beat_loss.item() * beats_pred.shape[0]\n",
        "        running_downbeat_loss += downbeat_loss.item() * downbeats_pred.shape[0]\n",
        "      \n",
        "      if tempo and beats:\n",
        "        if not downbeats:\n",
        "          if loss_network is None:\n",
        "            beat_loss = beat_criterion(beats_pred, beat_labels, pos_weight=pos_weight)\n",
        "          else:\n",
        "            beat_loss = losses[0].detach()\n",
        "          running_beat_loss += beat_loss.item() * beats_pred.shape[0]\n",
        "\n",
        "        if loss_network is None:\n",
        "          tempo_loss = tempo_criterion(tempo_pred, tempo_labels, pos_weight=pos_weight)\n",
        "        else:\n",
        "          tempo_loss = losses[2].detach() if downbeats else losses[1].detach()\n",
        "        running_tempo_loss += tempo_loss.item() * tempo_pred.shape[0]\n",
        "      \n",
        "      # backward + optimize\n",
        "      overall_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if loss_network is not None:\n",
        "        loss_optimizer.step()\n",
        "\n",
        "      # update prints\n",
        "      if update_frequency and (batch_idx + 1) % update_frequency == 0:\n",
        "        print(f\"loss after batch {batch_idx + 1} of epoch {epoch + 1}: \" \\\n",
        "              f\"{running_batch_loss / (update_frequency * len(train_loader.dataset) / len(train_loader)): .8f}\")\n",
        "        running_batch_loss = 0\n",
        "    \n",
        "    # end of epoch prints\n",
        "    print(f\"===== Training loss for epoch {epoch + 1}: {running_loss / len(train_loader.dataset): .8f} =====\")\n",
        "\n",
        "    if downbeats:\n",
        "      print(f\"Beat loss: {running_beat_loss / len(train_loader.dataset): .8f}\")\n",
        "      print(f\"Downbeat loss: {running_downbeat_loss / len(train_loader.dataset): .8f}\")\n",
        "    if tempo and beats:\n",
        "      if not downbeats:\n",
        "        print(f\"Beat loss: {running_beat_loss / len(train_loader.dataset): .8f}\")\n",
        "      print(f\"Tempo loss: {running_tempo_loss / len(train_loader.dataset): .8f}\")\n",
        "\n",
        "    # evaluate on validation\n",
        "    if evaluation_frequency and (epoch + 1) % evaluation_frequency == 0:\n",
        "      val_loss, val_beat_loss, val_downbeat_loss, val_tempo_loss, \\\n",
        "      val_beat_score, val_downbeat_score, val_tempo_score = evaluate(model, \n",
        "                                                                    device,\n",
        "                                                                    evaluation_loader, \n",
        "                                                                    loss_network=loss_network,\n",
        "                                                                    beats=beats, \n",
        "                                                                    downbeats=downbeats, \n",
        "                                                                    downbeat_weight=downbeat_weight, \n",
        "                                                                    tempo=tempo, \n",
        "                                                                    tempo_weight=tempo_weight, \n",
        "                                                                    tempo_range=tempo_range, \n",
        "                                                                    peak_mode=peak_mode, \n",
        "                                                                    peak_threshold=peak_threshold, \n",
        "                                                                    beat_pos_weight=beat_pos_weight, \n",
        "                                                                    max_filt_length=max_filt_length)\n",
        "      print(f'\\t===== Validation loss: {val_loss: .8f} =====')\n",
        "      output_data['train_losses'].append(running_loss / len(train_loader.dataset))\n",
        "      output_data['validation_losses'].append(val_loss)\n",
        "      if beats:\n",
        "        print(f'\\tValidation beat f-measure: {val_beat_score: .4f}')\n",
        "        output_data['beat_f1s'].append(val_beat_score)\n",
        "        if tempo:\n",
        "          output_data['beat_train_losses'].append(running_beat_loss / len(train_loader.dataset))\n",
        "          output_data['beat_val_losses'].append(val_beat_loss)\n",
        "      if downbeats:\n",
        "        print(f\"\\tValidation beat loss: {val_beat_loss: .8f}\")\n",
        "        print(f'\\tValidation downbeat f-measure: {val_downbeat_score: .4f}')\n",
        "        print(f\"\\tValidation downbeat loss: {val_downbeat_loss: .8f}\")\n",
        "        output_data['downbeat_f1s'].append(val_downbeat_score)\n",
        "        output_data['downbeat_train_losses'].append(running_downbeat_loss / len(train_loader.dataset))\n",
        "        output_data['downbeat_val_losses'].append(val_downbeat_score)\n",
        "      if tempo:\n",
        "        print(f\"\\tValidation tempo RMSE: {val_tempo_score: .8f}\")\n",
        "        output_data['tempo_RMSE'].append(val_tempo_score)\n",
        "        if beats:\n",
        "          if not downbeats:\n",
        "            print(f\"\\tValidation beat loss: {val_beat_loss: .8f}\")\n",
        "          print(f\"\\tValidation tempo loss: {val_tempo_loss: .8f}\")\n",
        "          output_data['tempo_train_losses'].append(running_tempo_loss / len(train_loader.dataset))\n",
        "          output_data['tempo_val_losses'].append(val_tempo_loss)\n",
        "      if loss_network is not None:\n",
        "        for name, param in loss_network.state_dict().items():\n",
        "          if name == 'weights':\n",
        "            print(f\"Trainable loss parameters: {name}, {param}\")\n",
        "            param = param.cpu()\n",
        "            output_data['beat loss weight'].append(param[0])\n",
        "            if downbeats:\n",
        "              output_data['downbeat loss weight'].append(param[1])\n",
        "            if tempo:\n",
        "              output_data['tempo loss weight'].append(param[-1])\n",
        "        # print(f\"Loss weights: {loss_network.weights}\")\n",
        "\n",
        "      # if the best validation performance so far, save the network to file \n",
        "      if(val_loss < best_validation_loss):\n",
        "        best_validation_loss = val_loss\n",
        "        # get full path\n",
        "        directory = \"/content/gdrive/MyDrive/Colab Notebooks/QM DL for music and audio/SavedModels\"\n",
        "        save_path = os.path.join(directory, save_name)\n",
        "        print('Saving best model')\n",
        "        torch.save(model.state_dict(), save_path) \n",
        "\n",
        "      # update scheduler\n",
        "      if scheduler is not None:\n",
        "        lr_before = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(val_loss)\n",
        "        lr_after = optimizer.param_groups[0]['lr']\n",
        "        if lr_before != lr_after:\n",
        "          print(f\"Learning rate reduced to {lr_after}\")\n",
        "\n",
        "  # save output data\n",
        "  log_root = \"/content/gdrive/MyDrive/Colab Notebooks/QM DL for music and audio/training_data\"\n",
        "  save_name = save_name + '.pkl'\n",
        "  log_name = os.path.join(log_root, save_name)\n",
        "\n",
        "  with open(log_name, 'wb') as f:\n",
        "    pickle.dump(output_data, f)\n",
        "\n",
        "  print(list(output_data.keys()))\n",
        "\n",
        "  return output_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc_tv-URDwJx"
      },
      "outputs": [],
      "source": [
        "# evaluation loop\n",
        "\n",
        "def evaluate(model, \n",
        "             device, \n",
        "             test_loader, \n",
        "             loss_network=None,\n",
        "             beats=True,\n",
        "             downbeats=False,\n",
        "             downbeat_weight=0.5,\n",
        "             tempo=False, \n",
        "             tempo_weight=0.5, \n",
        "             tempo_range=(10, 360), \n",
        "             peak_mode='max_filt', \n",
        "             peak_threshold=0.25, \n",
        "             max_filt_length=7,\n",
        "             beat_pos_weight=None):\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  running_loss = 0\n",
        "  running_beat_loss = 0\n",
        "  running_downbeat_loss = 0\n",
        "  running_tempo_loss = 0\n",
        "\n",
        "  running_beat_score = 0\n",
        "  running_downbeat_score = 0\n",
        "  running_tempo_score = 0\n",
        "\n",
        "  if loss_network is None:\n",
        "    # get loss functions\n",
        "    criteria = model.loss_functions()\n",
        "    overall_criterion = criteria['loss']\n",
        "\n",
        "    if beats:\n",
        "      beat_criterion = criteria['beat_loss']\n",
        "    if downbeats:\n",
        "      downbeat_criterion = criteria['downbeat_loss']\n",
        "    if tempo and beats:\n",
        "      tempo_criterion = criteria['tempo_loss']\n",
        "  else:\n",
        "    loss_network.eval()\n",
        "\n",
        "  # set positive label loss weightings\n",
        "  if loss_network is None:\n",
        "    pos_weight = None\n",
        "    if beat_pos_weight is not None:\n",
        "      pos_weight = torch.Tensor(beat_pos_weight).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, data in enumerate(test_loader):\n",
        "\n",
        "      # get samples and ground truth labels\n",
        "      x = data['pianoroll']\n",
        "      x = x.to(device)\n",
        "\n",
        "      # get labels\n",
        "      beat_labels = None\n",
        "      if beats:\n",
        "        beat_labels = data['beats']\n",
        "        beat_labels = torch.squeeze(beat_labels)\n",
        "        beat_labels = beat_labels.to(device)\n",
        "      downbeat_labels = None\n",
        "      if downbeats:\n",
        "        downbeat_labels = data['downbeast']\n",
        "        torch.squeeze(downbeat_labels)\n",
        "        downbeat_labels = downbeat_labels.to(device)\n",
        "      tempo_labels = None\n",
        "      if tempo:\n",
        "        tempo_labels = data['tempo']\n",
        "        torch.squeeze(tempo_labels)\n",
        "        tempo_labels = tempo_labels\n",
        "      \n",
        "      # get outputs\n",
        "      beats_pred, downbeats_pred, tempo_pred = model(x)\n",
        "\n",
        "      if beats:\n",
        "        beats_pred = torch.squeeze(beats_pred)\n",
        "      if downbeats:\n",
        "        downbeats_pred = torch.squeeze(downbeats_pred)\n",
        "      if tempo:\n",
        "        tempo_pred = torch.squeeze(tempo_pred)\n",
        "\n",
        "      # downsample tempo labels\n",
        "      if tempo:\n",
        "        tempo_labels = signal.resample(tempo_labels, tempo_pred.shape[1], axis=1)\n",
        "        tempo_labels = torch.Tensor(tempo_labels).to(device)\n",
        "\n",
        "      # calculate losses\n",
        "      if loss_network is None:\n",
        "        overall_loss = overall_criterion(beats_pred, beat_labels, downbeats_pred, downbeat_labels, \n",
        "                                        downbeat_weight, tempo_pred, tempo_labels, tempo_weight, \n",
        "                                        pos_weight=pos_weight)\n",
        "      else:\n",
        "        losses, overall_loss = loss_network(beats_pred, beat_labels, downbeats_pred, downbeat_labels, \n",
        "                                        tempo_pred, tempo_labels)\n",
        "      \n",
        "      running_loss += overall_loss.item() * (beats_pred.shape[0] if beats else tempo_pred.shape[0])\n",
        "\n",
        "      if downbeats:\n",
        "        if loss_network is None:\n",
        "          beat_loss = beat_criterion(beats_pred, beat_labels, pos_weight=pos_weight)\n",
        "          downbeat_loss = downbeat_criterion(downbeats_pred, downbeat_labels, \n",
        "                                             pos_weight=pos_weight)\n",
        "        else: \n",
        "          beat_loss = losses[0].detach()\n",
        "          downbeat_loss = losses[1].detach()       \n",
        "        running_beat_loss += beat_loss.item() * beats_pred.shape[0]\n",
        "        running_downbeat_loss += downbeat_loss.item() * downbeats_pred.shape[0]\n",
        "      \n",
        "      if tempo and beats:\n",
        "        if not downbeats:\n",
        "          if loss_network is None:\n",
        "            beat_loss = beat_criterion(beats_pred, beat_labels, pos_weight=pos_weight)\n",
        "          else:\n",
        "            beat_loss = losses[0].detach()\n",
        "          running_beat_loss += beat_loss.item() * beats_pred.shape[0]\n",
        "\n",
        "        if loss_network is None:\n",
        "          tempo_loss = tempo_criterion(tempo_pred, tempo_labels, pos_weight=pos_weight)\n",
        "        else:\n",
        "          tempo_loss = losses[2].detach() if downbeats else losses[1].detach()\n",
        "        running_tempo_loss += tempo_loss.item() * tempo_pred.shape[0]       \n",
        "      \n",
        "      # calculate f-scores for beats and downbeats\n",
        "      if beats:\n",
        "\n",
        "        # print(f\"Eval raw beats pred: {beats_pred}\")\n",
        "        # print(f\"Eval raw beats labels: {beat_labels}\")\n",
        "\n",
        "        beats_pred = torch.sigmoid(beats_pred)\n",
        "        beat_labels = beat_labels.cpu()\n",
        "        beats_pred = beats_pred.cpu()\n",
        "\n",
        "        for i in range(beat_labels.shape[0]):\n",
        "          # convert beat labels to times\n",
        "          labels = times_from_labels(beat_labels[i, :])\n",
        "          # peak picking to get predicted beat times\n",
        "          preds = peak_picker(beats_pred[i, :], sr=100, mode=peak_mode, min_bpm=tempo_range[0], \n",
        "                              max_bpm=tempo_range[1], threshold=peak_threshold, \n",
        "                              max_filt_length=max_filt_length)\n",
        "\n",
        "          if preds.size != 0:\n",
        "            running_beat_score += mir_eval.beat.f_measure(labels, preds)\n",
        "\n",
        "      if downbeats:\n",
        "        downbeats_pred = torch.sigmoid(downbeats_pred)\n",
        "        downbeat_labels = downbeat_labels.cpu()\n",
        "        downbeats_pred = downbeats_pred.cpu()\n",
        "\n",
        "        for i in range(downbeat_labels.shape[0]):\n",
        "          labels = times_from_labels(downbeat_labels[i, :])\n",
        "          preds = peak_picker(downbeats_pred[i, :], sr=100, mode='max_filt', min_bpm=tempo_range[0], \n",
        "                              max_bpm=tempo_range[1], threshold=peak_threshold, \n",
        "                              max_filt_length=max_filt_length)\n",
        "          \n",
        "          if preds.size != 0:\n",
        "            running_downbeat_score += mir_eval.beat.f_measure(labels, preds)\n",
        "\n",
        "      if tempo:\n",
        "        running_tempo_score += torch.sum(torch.abs(tempo_pred - tempo_labels)).item() / tempo_pred.shape[1]\n",
        "      \n",
        "  running_loss /= len(test_loader.dataset)\n",
        "  running_beat_loss /= len(test_loader.dataset)\n",
        "  running_downbeat_loss /= len(test_loader.dataset)\n",
        "  running_tempo_loss /= len(test_loader.dataset)\n",
        "\n",
        "  running_beat_score /= len(test_loader.dataset)\n",
        "  running_downbeat_score /= len(test_loader.dataset)\n",
        "  running_tempo_score = running_tempo_score / len(test_loader.dataset) * 60\n",
        "\n",
        "  return running_loss, running_beat_loss, running_downbeat_loss, running_tempo_loss, \\\n",
        "          running_beat_score, running_downbeat_score, running_tempo_score\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4k15xNgq6rz"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "def load_state(model, load_name):\n",
        "\n",
        "  directory = \"/content/gdrive/MyDrive/Colab Notebooks/QM DL for music and audio/SavedModels\" \n",
        "  path = os.path.join(directory, load_name)\n",
        "\n",
        "  model.load_state_dict(torch.load(path))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# load model output data\n",
        "def load_data(name):\n",
        "  directory = \"/content/gdrive/MyDrive/Colab Notebooks/QM DL for music and audio/training_data\" \n",
        "  name = name + '.pkl'\n",
        "  name = os.path.join(directory, name)\n",
        "\n",
        "  with open(name, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "  return data\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huNAnG_UEGum"
      },
      "outputs": [],
      "source": [
        "# For plotting learning curves and evaluation metrics\n",
        "\n",
        "def plot_training(update_rate, data, metrics, trim=0):\n",
        "\n",
        "  if type(data) == list:\n",
        "    num_plots = len(data)\n",
        "  else:\n",
        "    num_plots = 1\n",
        "    data = [data]\n",
        "\n",
        "  figwidth = min(20, 5 * len(data))\n",
        "\n",
        "  fig, ax = plt.subplots(1, num_plots, figsize=(figwidth, 4))\n",
        "\n",
        "  x_points = np.arange(len(data[0][metrics[0]])) * update_rate\n",
        "\n",
        "  if num_plots > 1:\n",
        "    for i, col in enumerate(ax):\n",
        "      for metric in metrics:\n",
        "        col.plot(x_points[trim:], data[i][metric][trim:], label=metric)\n",
        "\n",
        "        col.set_xlabel('epochs')\n",
        "        col.legend()\n",
        "  else:\n",
        "    for metric in metrics:\n",
        "      ax.plot(x_points[trim:], data[0][metric][trim:], label=metric)\n",
        "\n",
        "      ax.set_xlabel('epochs')\n",
        "      ax.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "def test(model, \n",
        "         device, \n",
        "         test_loader, \n",
        "         tempo_model=None,\n",
        "         sr=22050,\n",
        "         num_samples=1, \n",
        "         beats=True, \n",
        "         downbeats=False, \n",
        "         tempo=True, \n",
        "         tempo_range=(10, 360), \n",
        "         peak_mode='max_filt', \n",
        "         peak_threshold=0.25, \n",
        "         max_filt_length=7, \n",
        "         folder=None):\n",
        "\n",
        "  \"\"\"\n",
        "  Test a model on input samples.  \n",
        "  Synthesize to audion and plot results.  \n",
        "\n",
        "  Assumes a batch size of 1.  \n",
        "  \"\"\"\n",
        "\n",
        "  # ASAP dataset root\n",
        "  asap_directory = \"./asap-dataset\"\n",
        "\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "\n",
        "  if tempo_model is not None:\n",
        "    tempo_model.eval()\n",
        "    tempo_model.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, data in enumerate(test_loader):\n",
        "\n",
        "      if num_samples and idx >= num_samples:\n",
        "        break\n",
        "      \n",
        "      x = data['pianoroll']\n",
        "      x = x.to(device)\n",
        "\n",
        "      assert x.shape[0] == 1, \"Batch size must be 1 for testing\"\n",
        "\n",
        "      # get predictions\n",
        "      beats_pred, downbeats_pred, tempo_pred = model(x)\n",
        "\n",
        "      if tempo_model is not None:\n",
        "        _, _, tempo_pred = tempo_model(x)\n",
        "\n",
        "      # get labels\n",
        "      beat_labels = None\n",
        "      if beats:\n",
        "        beat_labels = torch.squeeze(data['beats']).numpy()\n",
        "      downbeat_labels = None\n",
        "      if downbeats:\n",
        "        downbeat_labels = torch.squeeze(data['downbeast']).numpy()\n",
        "      tempo_labels = None\n",
        "      if tempo:\n",
        "        tempo_labels = data['tempo']\n",
        "\n",
        "      # get MIDI track\n",
        "      path = data['original_name'][0]\n",
        "      print(f\"Testing track {path} ... \")\n",
        "      piece = pm.PrettyMIDI(os.path.join(asap_directory, path))\n",
        "\n",
        "      # synthesize audio\n",
        "      waveform = piece.synthesize(fs=sr, wave=signal.square)\n",
        "      start = data['start'][0].item()\n",
        "      start = int(start / 100 * sr)\n",
        "      end = int(4349 / 100 * sr)\n",
        "      waveform = waveform[start: start + end]\n",
        "\n",
        "      print(start)\n",
        "\n",
        "      # get click track\n",
        "      beat_preds = peak_picker(beats_pred, sr=100, mode=peak_mode, min_bpm=tempo_range[0], \n",
        "                          max_bpm=tempo_range[1], threshold=peak_threshold, \n",
        "                          max_filt_length=max_filt_length)\n",
        "      clicks = librosa.clicks(times=beat_preds, sr=sr)\n",
        "\n",
        "      # put click onto audio\n",
        "      audio_len = len(waveform)\n",
        "      if len(clicks) > audio_len:\n",
        "        clicks = clicks[:audio_len]\n",
        "      elif len(clicks) < audio_len:\n",
        "        pad = np.zeros(audio_len - len(clicks))\n",
        "        clicks = np.concatenate((clicks, pad))\n",
        "      output = waveform + clicks\n",
        "\n",
        "      # save audio track\n",
        "      save_dir = \"/content/gdrive/MyDrive/Colab Notebooks/QM DL for music and audio/audio\"\n",
        "      if folder:\n",
        "        save_dir = os.path.join(save_dir, folder)\n",
        "      save_name = data['name'][0][:-4] + 'wav'\n",
        "      save_path = os.path.join(save_dir, save_name)\n",
        "      sf.write(save_path, output, sr)\n",
        "\n",
        "      # calculate metrics\n",
        "      labels = times_from_labels(beat_labels)\n",
        "      beat_f1 = mir_eval.beat.f_measure(labels, beat_preds)\n",
        "\n",
        "      if downbeats:\n",
        "        dblabels = times_from_labels(downbeat_labels)\n",
        "        downbeat_preds = peak_picker(downbeats_pred, sr=100, mode=peak_mode, min_bpm=tempo_range[0], \n",
        "                                     max_bpm=tempo_range[1], threshold=peak_threshold, \n",
        "                                     max_filt_length=max_filt_length)\n",
        "        downbeat_f1 = mir_eval.beat.f_measure(dblabels, downbeat_preds)       \n",
        "      \n",
        "      tempo_pred = torch.squeeze(tempo_pred).numpy()\n",
        "      tempo_labels = signal.resample(tempo_labels, tempo_pred.shape[0], axis=1)\n",
        "      tempo_accuracy = np.mean(np.abs(tempo_pred - tempo_labels))\n",
        "\n",
        "      print(f\"Beat f1: {beat_f1}\")\n",
        "      if downbeats:\n",
        "        print(f\"Downbeat f1: {downbeat_f1}\")\n",
        "      print(f\"Tempo Accuracy: {tempo_accuracy}\")\n",
        "\n",
        "      x = x.numpy()\n",
        "      x = np.squeeze(x)\n",
        "      tempo_labels = np.squeeze(tempo_labels) * 60\n",
        "      tempo_pred = np.squeeze(tempo_pred) * 60\n",
        "\n",
        "      # show graphs\n",
        "      graph(pianoroll=x, \n",
        "            beats=None, \n",
        "            beats_pred=None,\n",
        "            downbeats=None, \n",
        "            downbeats_pred=None, \n",
        "            tempo=None, \n",
        "            tempo_pred=None,\n",
        "            min_tempo=10,\n",
        "            sr=100, \n",
        "            note_range=(0, 88), \n",
        "            time_range=None)\n",
        "      \n",
        "      graph(pianoroll=None, \n",
        "            beats=labels, \n",
        "            beats_pred=beat_preds,\n",
        "            downbeats=None, \n",
        "            downbeats_pred=None, \n",
        "            tempo=None, \n",
        "            tempo_pred=None,\n",
        "            min_tempo=10,\n",
        "            sr=100, \n",
        "            note_range=(0, 88), \n",
        "            time_range=None)\n",
        "\n",
        "      if downbeats:\n",
        "        graph(pianoroll=None, \n",
        "              beats=None, \n",
        "              beats_pred=None,\n",
        "              downbeats=downbeat_labels, \n",
        "              downbeats_pred=downbeat_preds, \n",
        "              tempo=None, \n",
        "              tempo_pred=None,\n",
        "              min_tempo=10,\n",
        "              sr=100, \n",
        "              note_range=(0, 88), \n",
        "              time_range=None)\n",
        "\n",
        "      graph(pianoroll=None, \n",
        "            beats=None, \n",
        "            beats_pred=None,\n",
        "            downbeats=None, \n",
        "            downbeats_pred=None, \n",
        "            tempo=tempo_labels, \n",
        "            tempo_pred=tempo_pred,\n",
        "            min_tempo=10,\n",
        "            sr=100, \n",
        "            note_range=(0, 88), \n",
        "            time_range=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "2HrrC_NqxCCp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Joint-Beat-and-Tempo-Tracker.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJp8W7uy/mjpdkR4ltB4xw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}